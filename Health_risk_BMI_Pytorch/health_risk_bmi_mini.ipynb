{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5239484",
   "metadata": {},
   "source": [
    "# Health Risk Prediction (Mini Version)\n",
    "\n",
    "This mini project trains a simple regression model to **predict BMI** from lifestyle features using **PyTorch** if available, or a **NumPy fallback** if PyTorch is not installed.\n",
    "\n",
    "**Steps covered:**\n",
    "1. Dataset Generation (10–15 rows)\n",
    "2. Normalization (custom StandardScaler)\n",
    "3. Training (PyTorch Linear Regression or NumPy GD)\n",
    "4. Evaluation (MSE and R²)\n",
    "5. Prediction for a new sample (7h sleep, 7000 steps, 2200 calories)\n",
    "\n",
    "> You can run all cells in order. If `torch` isn’t available in your environment, the notebook will automatically use the NumPy implementation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c637e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "# ---------- Helper: simple StandardScaler (no sklearn needed) ----------\n",
    "class StandardScalerLite:\n",
    "    def fit(self, X):\n",
    "        self.mean_ = X.mean(axis=0)\n",
    "        self.std_ = X.std(axis=0)\n",
    "        self.std_[self.std_ == 0] = 1.0\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return (X - self.mean_) / self.std_\n",
    "    def fit_transform(self, X):\n",
    "        return self.fit(X).transform(X)\n",
    "    def inverse_transform(self, X_scaled):\n",
    "        return (X_scaled * self.std_) + self.mean_\n",
    "\n",
    "# ---------- Step 1: create dummy dataset ----------\n",
    "np.random.seed(42)\n",
    "sleep = np.random.randint(5, 9, size=12)\n",
    "steps = np.random.randint(3500, 12000, size=12)\n",
    "cal   = np.random.randint(1600, 3200, size=12)\n",
    "\n",
    "bmi   = (\n",
    "    28.0\n",
    "    - 0.22 * (sleep - 6.5)\n",
    "    - 0.0006 * (steps - 7000)\n",
    "    + 0.0020 * (cal - 2200)\n",
    "    + np.random.normal(0, 0.7, size=12)\n",
    ")\n",
    "\n",
    "X = np.column_stack([sleep, steps, cal]).astype(float)\n",
    "y = bmi.astype(float).reshape(-1, 1)\n",
    "\n",
    "print(\"Sample rows:\\n\", np.column_stack([X, y])[:5])\n",
    "\n",
    "# ---------- Step 2: normalization ----------\n",
    "scaler = StandardScalerLite()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "y_mean, y_std = y.mean(), y.std()\n",
    "if y_std == 0:\n",
    "    y_std = 1.0\n",
    "y_scaled = (y - y_mean) / y_std\n",
    "\n",
    "# ---------- Metrics ----------\n",
    "def mse(y_true, y_pred):\n",
    "    return float(np.mean((y_true - y_pred) ** 2))\n",
    "\n",
    "def r2_score(y_true, y_pred):\n",
    "    y_true = y_true.reshape(-1, 1)\n",
    "    y_pred = y_pred.reshape(-1, 1)\n",
    "    ss_res = float(np.sum((y_true - y_pred) ** 2))\n",
    "    ss_tot = float(np.sum((y_true - np.mean(y_true)) ** 2))\n",
    "    return 1.0 - (ss_res / ss_tot if ss_tot != 0 else 0.0)\n",
    "\n",
    "# ---------- Step 3: Train (PyTorch if available else NumPy) ----------\n",
    "use_torch = False\n",
    "try:\n",
    "    import torch\n",
    "    use_torch = True\n",
    "except Exception as e:\n",
    "    use_torch = False\n",
    "\n",
    "if use_torch:\n",
    "    import torch\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "    X_t = torch.tensor(X_scaled, dtype=torch.float32)\n",
    "    y_t = torch.tensor(y_scaled, dtype=torch.float32)\n",
    "\n",
    "    model = torch.nn.Sequential(torch.nn.Linear(3, 1))\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.05)\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "    epochs = 800\n",
    "    losses = []\n",
    "    for ep in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(X_t)\n",
    "        loss = loss_fn(preds, y_t)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(float(loss.item()))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        y_pred_scaled = model(X_t).numpy()\n",
    "\n",
    "    y_pred = (y_pred_scaled * y_std) + y_mean\n",
    "    framework = \"PyTorch\"\n",
    "    final_train_loss = losses[-1]\n",
    "else:\n",
    "    np.random.seed(42)\n",
    "    n, d = X_scaled.shape\n",
    "    W = np.random.randn(d, 1) * 0.01\n",
    "    b = np.zeros((1,))\n",
    "    lr = 0.05\n",
    "    epochs = 800\n",
    "    losses = []\n",
    "\n",
    "    for ep in range(epochs):\n",
    "        y_hat = X_scaled @ W + b\n",
    "        loss = np.mean((y_hat - y_scaled)**2)\n",
    "        losses.append(float(loss))\n",
    "        dW = (2.0/n) * (X_scaled.T @ (y_hat - y_scaled))\n",
    "        db = (2.0/n) * np.sum(y_hat - y_scaled, axis=0)\n",
    "        W -= lr * dW\n",
    "        b -= lr * db\n",
    "\n",
    "    y_pred_scaled = X_scaled @ W + b\n",
    "    y_pred = (y_pred_scaled * y_std) + y_mean\n",
    "    framework = \"NumPy (fallback)\"\n",
    "    final_train_loss = losses[-1]\n",
    "\n",
    "# ---------- Step 4: Evaluation ----------\n",
    "final_mse = mse(y, y_pred)\n",
    "final_r2  = r2_score(y, y_pred)\n",
    "\n",
    "print(f\"Framework used     : {framework}\")\n",
    "print(f\"Final train loss   : {final_train_loss:.6f} (on scaled y)\")\n",
    "print(f\"MSE (original y)   : {final_mse:.6f}\")\n",
    "print(f\"R^2 (original y)   : {final_r2:.6f}\")\n",
    "\n",
    "# ---------- Step 5: Predict on a new sample ----------\n",
    "new_sample = np.array([[7.0, 7000.0, 2200.0]])\n",
    "new_sample_scaled = scaler.transform(new_sample)\n",
    "\n",
    "if use_torch:\n",
    "    with torch.no_grad():\n",
    "        pred_scaled = model(torch.tensor(new_sample_scaled, dtype=torch.float32)).numpy()\n",
    "else:\n",
    "    pred_scaled = new_sample_scaled @ W + b\n",
    "\n",
    "pred_bmi = float((pred_scaled * y_std) + y_mean)\n",
    "print(f\"New sample (7h, 7000 steps, 2200 cal) -> Predicted BMI: {pred_bmi:.3f}\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
